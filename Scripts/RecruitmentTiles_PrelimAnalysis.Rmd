---
title: "RecruitmentTiles_PrelimAnalysis"
author: "Hannah Merges"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning=FALSE, 
                      message=FALSE)
```

## Load libraries 
```{r}
library(segmented)
library(plotrix)
library(gridExtra)
library(LoLinR)
library(lubridate)
library(chron)
library(patchwork)
library(tidyverse)
library(here)
library(PNWColors)
library(ggrepel)
library(reshape2)
library(viridis)
library(car)
library(GGally)
library(corrplot)
library(PNWColors)
library(seacarb)
library(broom)
library(calecopal)
library(ggridges)
library(agricolae)
library(lme4)
library(lmerTest)
library(modelsummary)
library(tidymodels)
library(flextable)
library(performance)
library(agricolae)
library(ggeffects)
library(sjPlot)
library(patchwork)
library(broom)
library(purrr)
library(nlstools)
library(stringr)
library(emmeans)
library(MuMIn)
library(forcats)
library(ggmap)
library(maptools)
library(kriging)
library(ggnewscale)
library(wql)
#library(glue)
#library(curl)
library(pscl)
```

## Bring in datasets
```{r}
RecruitmentCounts <- read_csv(here("Data","Recruitment_Tiles_Counts.csv"))
AllChemData <- read_csv("https://raw.githubusercontent.com/njsilbiger/MooreaSGD_site-selection/main/Data/August2021/Allbiogeochemdata_QC.csv")
PercentCover <- read_csv("https://raw.githubusercontent.com/dbarnas/Community_Functional_Diversity/main/Data/Full_Metadata.csv")
pH <- read_csv(here("Data", "pHSlope.csv"))
```

## join dfs 
```{r}

full_CommunityRecruit <- PercentCover %>% 
  left_join(RecruitmentCounts, join_by(CowTagID, Location))

full_envdataRecruits <- full_CommunityRecruit %>% 
  left_join(AllChemData, join_by(Location, CowTagID, lat, lon))
```

# tidy this now fully complete and joined dataset 
```{r}
full_envdataRecruits <- full_envdataRecruits %>% 
  select(!c("del15N", "adj_CT_depth_cm", "meanRugosity", "Top_Plate_ID", "Bottom_Plate_ID", "Jamie_Plate_ID", "Time", "DateTime", "M_C":"Lignin_Like"))

#full_envdataRecruits <- full_envdataRecruits %>% 
  #select()
# should I get rid of extra rows from seeps and rock wall, etc ??? 

```

# group and average the day/night and high/low tide measurements 
```{r}

full_envdataRecruits2 <- full_envdataRecruits %>% 
  select(!c("Date", "Tide", "Day_Night", "Plate_Seep")) %>% 
  group_by(CowTagID) %>% 
  mutate(avgSal=mean(Salinity, na.rm=TRUE), 
         avgTemp=mean(Temperature, na.rm=TRUE), 
         avgTA=mean(TA, na.rm=TRUE), 
         avgpH=mean(pH, na.rm=TRUE), 
         avgPhosphate=mean(Phosphate_umolL, na.rm=TRUE), 
         avgSilicate=mean(Silicate_umolL, na.rm=TRUE),
         avgNN=mean(NN_umolL, na.rm=TRUE), 
         avgAmmonia=mean(Ammonia_umolL, na.rm=TRUE))

### not sure if I did this correctly??? maybe I could make this tidier? lots of extra rows due to date, tide, and day/night 
  
```

## tidy the df a bit more to start visualizing with plots 
### mainly pivoting the recruitment counts 
```{r}
## pivot dataset and group together 
Recruits_pivoted <- RecruitmentCounts %>% 
  group_by(CowTagID, Location, SGD_level, Side) %>% 
  pivot_longer(cols="Pocilloporidae":"Other", names_to = "recruit_counts") %>% 
  summarize(sum_total=sum(value)) 

### getting incorrect values if you FULL, joined dataframe because too many rows per CowTag bc of other data -- fix this -- for right now, using just the recruitment count df 

totalcounts_tilelocation <- Recruits_pivoted %>% 
  pivot_wider(names_from = Side, values_from = sum_total) %>% 
  mutate(total_count = Bottom + Side + Top) ##R recognizes double quotes as characters, so just add the literal column name 


```

## these plots show rough SGD estimate, taken from silicate kriging plot with total recruit counts per plate 

```{r}
## try to plot basics for Varari 
### this plot 
Recruits_by_SGD_level_V <- totalcounts_tilelocation %>% 
  filter(Location=="Varari") %>% 
  ggplot(aes(x=reorder(CowTagID, -total_count), 
             y=total_count, 
             fill=SGD_level)) + 
  geom_col() + 
  theme_blank()  + 
  scale_fill_manual(values=c("darkseagreen4", "darkgoldenrod2", "coral4"), 
                    limits=c("Low", "Mid", "High")) + 
   labs(title="Varari", 
       x="CowTag_ID", 
       y="Settler Counts")
  
Recruits_by_SGD_level_V

## for Cabral 
Recruits_by_SGD_level_C <- totalcounts_tilelocation %>% 
  filter(Location=="Cabral") %>% 
  ggplot(aes(x=reorder(CowTagID, -total_count), 
             y=total_count, 
             fill=SGD_level)) + 
  geom_col() + 
  theme_blank() + 
  scale_fill_manual(values=c("darkseagreen4", "darkgoldenrod2", "coral4"), 
                    limits=c("Low", "Mid", "High")) + 
  labs(title="Cabral", 
       x="CowTag_ID", 
       y="Settler Counts")
  
Recruits_by_SGD_level_C


Recruits_by_SGD_level_C + Recruits_by_SGD_level_V
ggsave(here("Outputs", "RecruitmentTiles", "RecruitbySGDestimates.jpg"), 
       width = 14, height = 10)


### try with SE bars  ?? 
totalcounts_tilelocation2 <- totalcounts_tilelocation %>% 
  group_by(SGD_level, CowTagID, Location, total_count) %>% 
  summarize(mean_recruit = mean(total_count, na.rm=TRUE), 
            se_recruit = sd(total_count, na.rm=TRUE)/sqrt(n())) %>% 
  ungroup() 
 # geom_errorbar(aes(ymin=mean_settled-se_settled, ymax=mean_settled+se_settled), width=0.1) 


################################
### associated models 
#################################

## trying to run ANOVA to see if there is any significance between treatments and between sites  
Varari_recruitment <- totalcounts_tilelocation %>% 
   filter(Site=="Varari")

anova_recruits_Varari <- aov(total_count ~ SGD_level, data=Varari_recruitment)
summary(anova_recruits_Varari)
emmeans(anova_recruits_Varari, pairwise~"SGD_level", adjust="Tukey")

Cabral_recruitment <- totalcounts_tilelocation %>% 
   filter(Site=="Cabral")

anova_recruits_Cabral <- lm(total_count ~ SGD_level, data=Cabral_recruitment) 
summary(anova_recruits_Cabral)
emmeans(anova_recruits_Cabral, pairwise~"SGD_level", adjust="Tukey")

anova_recruits <- aov(total_count ~ SGD_level + Site, data=totalcounts_tilelocation)
summary(anova_recruits)
emmeans(anova_recruits, pairwise~"SGD_level", adjust="Tukey")
```


## want to look at tile data for each of the env parameters 

## start with percent cover data 
### try with a regular barlplot and then stacked barplot because part out of whole 
```{r}
full_envdataRecruits3 <- full_envdataRecruits2 %>% 
  pivot_longer(cols = LiveCoral:Sand, names_to = "comm_type", values_to = "percent_cover") %>% 
  group_by(comm_type, CowTagID, Location) %>% 
  summarize(percent_total=sum(percent_cover)) 
  
full_envdataRecruits4 <- full_envdataRecruits3 %>% 
  pivot_wider(names_from = CowTagID, values_from = percent_total) 

full_envdataRecruits4 <- full_envdataRecruits4 %>% 
  select(!starts_with(c("Reef","RockWall","Sand"))) %>% ## gets rid of the extra data that I don't need - columns that start with Reef, Rockwall, or Sand 
  pivot_longer(cols = C1:VSEEP, names_to = "CowTagID", values_to = "percent_cover") %>% 
  drop_na(percent_cover)

#### now plot 
commcomp_plot_V <- full_envdataRecruits4 %>% 
  filter(Location=="Varari") %>% 
  ggplot(aes(x=factor(CowTagID, level = c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9", "V10", "V11", "V12", "V13", "V14", "V15", "V16", "V17", "V18", "V19", "V20", "VSEEP")),
             y=percent_cover,
             fill=comm_type)) + 
  geom_bar(position="stack", stat="identity") + 
  theme_blank() + 
  scale_fill_manual(values=c("darkolivegreen", "coral", "azure4", "burlywood3"),
                    limits=c("DeadCoral", "LiveCoral", "Rubble", "Sand")) + 
  labs(title="Varari", 
       x="CowTag_ID", 
       y="% cover of Comm Types") + 
  theme(axis.text.x = element_text(angle = 45))
  
  
commcomp_plot_V

## for Cabral 
commcomp_plot_C <- full_envdataRecruits4 %>% 
  filter(Location=="Cabral") %>% 
  ggplot(aes(x=factor(CowTagID, level = c("C1", "C2", "C3", "C4", "C5", "C6", "C7", "C8", "C9", "C10", "C11", "C12", "C13", "C14", "C15", "C16", "C17", "C18", "C19", "C20", "CSEEP")),
             y=percent_cover,
             fill=comm_type)) + 
  geom_bar(position="stack", stat="identity") + 
  theme_blank() + 
  scale_fill_manual(values=c("darkolivegreen", "coral", "azure4", "burlywood3"),
                    limits=c("DeadCoral", "LiveCoral", "Rubble", "Sand")) + 
  labs(title="Cabral", 
       x="CowTag_ID", 
       y="% cover of Comm Types") + 
  theme(axis.text.x = element_text(angle = 45))
  
commcomp_plot_C




commcomp_plot_C + commcomp_plot_V


```

## create a scatterplot to show comm comp data vs total number of recruits on tiles at each of these sites 

```{r}

## need to summarize totals and 

commComp_recruit_plot <- full_envdataRecruits2 
  


```














## Try a kriging plot 
Do kriging plot with number of recruits on tiles along percent cover of certain habitat - do live coral vs sand vs dead coral vs rubble for rn 
```{r}
#####################################
## first part is mapping
####################################


# mean lat and long for the maps
MeanGPS<-latlong %>%
  group_by(Location) %>%
  summarise(lon = median(lon, na.rm = TRUE) + 0.0002, # add 0.0002 to fit all on map at zoom 19
            lat = median(lat, na.rm = TRUE))
SiteGPS<-latlong %>%
  group_by(Location, CowTagID) %>%
  summarise(lon = mean(lon, na.rm = TRUE),
            lat = mean(lat, na.rm = TRUE))

# Base Maps
VarariBaseMap<-get_map(MeanGPS %>%
                         filter(Location == "Varari") %>%
                         select(lon,lat),
                       maptype = 'satellite', zoom = 19)
CabralBaseMap<-get_map(MeanGPS %>%
                         filter(Location == "Cabral") %>%
                         select(lon,lat),
                       maptype = 'satellite', zoom = 18)


### Make a spatial kriging file with polygon layers ####
### Bring in the polygons for the sites
#Varari
V_kml <- getKMLcoordinates(kmlfile=here("Data","Polygons","Varari_Polygon.kml"), ignoreAltitude=T)
#Cabral
C_kml <- getKMLcoordinates(kmlfile=here("Data","Polygons","Cabral_Polygon.kml"), ignoreAltitude=T)

```

```{r}
### IF USING RANGES
# Get the data ranges for all variables to cleaner plots
# DataRange <-AllChemData %>%
#   filter(Plate_Seep == "Plate") %>%
#   select(-Temperature)%>%
#   group_by(Location) %>%
#   summarise_if(is.numeric, range, na.rm = TRUE) %>%
#   ungroup() %>%
#   mutate(min_max = c("min","max","min","max")) %>%
#   pivot_longer(cols = Salinity:Ammonia_umolL, names_to = "Parameters", values_to = "Values") %>%
#   pivot_wider(names_from = min_max, values_from = Values) %>%
#   select(Location, Parameters, min, max)
# mins<-DataRange %>%
#   filter(is.na(max)) %>%
#   select(-max)
#
# maxs<-DataRange %>%
#   filter(is.na(min)) %>%
#   select(-min)
#
# min_max<-left_join(mins,maxs)

```

```{r}
# pivot data to long form
nData <- latlong %>%
  pivot_longer(cols = 11:18, names_to = "Parameters", values_to = "Values")

# make a function to do all the krigings
Krig_function <-function(dat_in = data, Lat = "lat", Lon = "lon", Param = "Values", poly ) {

  dat <- dat_in[,c(Lon, Lat, Param)]
  names(dat) <- c('Lon', 'Lat', 'Param')


  dat<-dat%>%
    drop_na()

  x <- dat$Lon
  y <- dat$Lat
  z <-dat$Param

  krig1 <- kriging(x, y, z, pixels=500, polygons=poly, lags = 3) ###pixels controls how fine or course you want the prediction data frame to be
  krig2 <- krig1$map

return(krig2)
}

#temporary code
# a <- nData %>%
#   filter(Location == "Varari") %>%
#   filter(Parameters == 'Silicate_umolL') %>%
#   group_nest(Parameters) %>%
#   #select(lat, lon, Values) %>%
#   kriging(x = 'lon', y = 'lat', response = 'Values', polygons = V_kml) #, pixels = 500, lags = 3)
# a$map


# And do it "safely": skipping NAs without breaking the code
Krig_function_safe<-safely(Krig_function)
```



```{r}
# nest by all parameters to make it easy to plot all types of maps
# Varari
Varari_kriging <- nData %>%
  filter(Location == "Varari") %>%
  filter(CowTagID != "VSEEP") %>% # only plate data because seep skeps the maps
#   select(-Temperature)%>% # this is temporary until we get the temperature data entered
#   filter(Plate_Seep == "Plate", # only plot the plates because the seep samples skew the maps
#          Location == "Varari") %>%
  droplevels()%>%
#   pivot_longer(cols = Salinity:Ammonia_umolL, names_to = "Parameters", values_to = "Values") %>%
#   select(lat, lon, Tide, Day_Night, Date, Parameters, Values) %>% # select the values that are important for the kriging
#  group_nest(Tide, Day_Night, Date, Parameters) %>% # group by parameters
  select(-c(Location,CowTagID)) %>%
  group_nest(Parameters) %>% # group by parameters
  mutate(preds = map(data, ~Krig_function_safe(dat_in = .x, poly = V_kml)), # run the function for every nested group (i.e., Parameter)
      #   preds = map(preds, head, -1), # remove the error column
       #  preds = map(preds, flatten_df), # flatten back to a tibble
 # mutate(preds = unlist(preds))
         longname = Parameters,
         plots = map2(preds, longname, ~ggmap(VarariBaseMap)+ # map2 maps over multiple inputs simultaneously, in parallel
                        geom_point(data=.x$result, aes(x=x, y=y, colour=pred), size=4, alpha=0.5) +
                        geom_point(data = SiteGPS %>% filter(Location == 'Varari'), aes(x=lon, y=lat))+
                        scale_color_viridis_c(" ", option = "plasma") +
                        coord_sf() +
                        theme(axis.line=element_blank(),
                              axis.text.x=element_blank(),
                              axis.text.y=element_blank(),
                              axis.ticks=element_blank(),
                              axis.title.x=element_blank(),
                              axis.title.y=element_blank(),
                              panel.grid.major = element_line(color = 'white', linetype = "dashed",size = 0.5),
                              plot.background=element_rect(fill='white')) +
                        ggtitle(glue("Varari: {.y}"))))
```



## STATS TESTS - USE A LOGISTIC REGRESSION 
```{r}
## need to tidy the data for this first 
# change the total counts to all corals and then alive or dead --> how? 
# need to add comm comp data 

# for sand community 
model1 <- glm(Dead_Alive ~ Sand, family = binomial(link="logit"), data=RecruitmentCounts)
Anova(model1)

# for coral community 
model2 <- glm(Dead_Alive ~ Live_Coral, family = binomial(link="logit"), data=RecruitmentCounts)
Anova(model2)

# for dead coral community 
model3 <- glm(Dead_Alive ~ Dead_Coral, family = binomial(link="logit"), data=RecruitmentCounts)
Anova(model3)


# for rubble community 
model4 <- glm(Dead_Alive ~ Rubble, family = binomial(link="logit"), data=RecruitmentCounts)
Anova(model4)





```

## after models are run, want to get R2 values for them 

```{r}
# We'd also like to get a pseudo R^2 value that tells us how much of the variance in settler death/life is explained by the comm

pR2(model1) # gives pseudo R^2
# The last three columns are all different types of pseudo-R^2s --> usually use McFadden's rho-square


```


## plot the data from the models 
```{r}
plot(RecruitmentCounts$Sand, RecruitmentCounts$Dead_Alive,
     xlab="Community Composition of Sand, etc ",
     ylab="Probability of Settler Alive or Dead")
curve(predict(model1,data.frame(Sand=x),type="resp"),add=TRUE) # draws a curve based on prediction from logistic regression model

# Here's some code for making a nicer plot with ggplot
to.predict <- data.frame(Sand=seq(5,65,5))
predicted.data <- cbind(to.predict, as.data.frame(predict(model1, newdata=to.predict, type="response", se=TRUE)))

predicted.data

ggplot(lizards, aes(x=PA_ratio, y=Present)) +
  geom_point() +
  geom_line(data = predicted.data, aes(x=PA_ratio, y=fit), color="blue") +
  labs(x="Perimeter:Area Ratio", y="Presence of Lizards")
  theme_bw() 
```








